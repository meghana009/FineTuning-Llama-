# FineTuning-Llama
Finetuned Llama2-7b for optimizing memory overhead using gradient checkpointing, gradient accummulation, mixed-precision training and LoRA on Alpaca Dataset

The project was collaborated and you can find it at: https://github.com/surajbidnur/llama/tree/main
